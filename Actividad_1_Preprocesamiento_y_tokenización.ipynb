{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64464af",
   "metadata": {},
   "source": [
    "# Actividad 1: Preprocesamiento y Tokenización\n",
    "---\n",
    "En esta actividad, realizaremos los siguientes pasos:\n",
    "\n",
    "1. Cargar el archivo de datos de noticias.\n",
    "2. Realizar el preprocesamiento del texto, que incluye:\n",
    "   - Convertir el texto a minúsculas.\n",
    "   - Eliminar puntuación.\n",
    "   - Eliminar números.\n",
    "   - Eliminar espacios en blanco adicionales.\n",
    "3. Tokenizar el texto en palabras individuales.\n",
    "4. Eliminar stop words del texto tokenizado.\n",
    "5. Calcular TF-IDF para representar el texto como vectores numéricos.\n",
    "6. Generar embeddings de palabras utilizando Word2Vec.\n",
    "\n",
    "\n",
    "## Librerías\n",
    "\n",
    "Para esta actividad, necesitaremos las siguientes librerías:\n",
    "\n",
    "- pandas: Para cargar y manipular los datos.\n",
    "- numpy: Para realizar operaciones numéricas.\n",
    "- nltk: Para realizar el preprocesamiento y tokenización del texto.\n",
    "- gensim: Para generar los embeddings de palabras.\n",
    "\n",
    "Este proyecto usa Python 3.10 y usa poetry para manejar las dependencias. Para instalar las dependencias, ejecute `poetry install` en la carpeta raíz del proyecto. Para más información sobre poetry, consulte la [documentación oficial](https://python-poetry.org/docs/).\n",
    "\n",
    "Si no quiere usar poetry, puede instalar las dependencias manualmente usando pip:\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy scikit-learn nltk gensim scipy openpyxl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8aca610-e021-417a-8f4c-4265fe2ddb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.14\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9ac65",
   "metadata": {},
   "source": [
    "### NLTK\n",
    "\n",
    "Nltk requiere que descarguemos algunos recursos adicionales. Para hacerlo, ejecute el siguiente código:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ee605c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Germán\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Germán\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Germán\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Columna1</th>\n",
       "      <th>Enlaces</th>\n",
       "      <th>Título</th>\n",
       "      <th>info</th>\n",
       "      <th>contenido</th>\n",
       "      <th>Etiqueta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.eltiempo.com/agresion-contra-un-op...</td>\n",
       "      <td>Operador de grúa quedó inconsciente tras agres...</td>\n",
       "      <td>El conductor de una moto le lanzó el casco y p...</td>\n",
       "      <td>Las autoridades están buscando al conductor de...</td>\n",
       "      <td>colombia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>Usaquén, primera en infracciones por mal parqueo</td>\n",
       "      <td>La localidad ocupa el primer lugar en comparen...</td>\n",
       "      <td>\"Los andenes son para los peatones\", reclama e...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>'Me atracaron y vi un arma que me heló la sang...</td>\n",
       "      <td>Un ciudadano relata cómo cuatro hombres lo rob...</td>\n",
       "      <td>A las 7 de la noche me había quedado de encont...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>Escoltas mal estacionados, dolor de cabeza de ...</td>\n",
       "      <td>Las zonas de restaurantes se convierten en par...</td>\n",
       "      <td>Atravesados. Eso es lo que se les pasa por la ...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>Radicado primer proyecto que autorizaría union...</td>\n",
       "      <td>El representante de 'la U', Miguel Gómez, dijo...</td>\n",
       "      <td>“Estamos proponiendo la figura de un contrato ...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Columna1                                            Enlaces  \\\n",
       "0         0  https://www.eltiempo.com/agresion-contra-un-op...   \n",
       "1         1  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "2         2  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "3         3  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "4         4  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "\n",
       "                                              Título  \\\n",
       "0  Operador de grúa quedó inconsciente tras agres...   \n",
       "1   Usaquén, primera en infracciones por mal parqueo   \n",
       "2  'Me atracaron y vi un arma que me heló la sang...   \n",
       "3  Escoltas mal estacionados, dolor de cabeza de ...   \n",
       "4  Radicado primer proyecto que autorizaría union...   \n",
       "\n",
       "                                                info  \\\n",
       "0  El conductor de una moto le lanzó el casco y p...   \n",
       "1  La localidad ocupa el primer lugar en comparen...   \n",
       "2  Un ciudadano relata cómo cuatro hombres lo rob...   \n",
       "3  Las zonas de restaurantes se convierten en par...   \n",
       "4  El representante de 'la U', Miguel Gómez, dijo...   \n",
       "\n",
       "                                           contenido  Etiqueta  \n",
       "0  Las autoridades están buscando al conductor de...  colombia  \n",
       "1  \"Los andenes son para los peatones\", reclama e...   archivo  \n",
       "2  A las 7 de la noche me había quedado de encont...   archivo  \n",
       "3  Atravesados. Eso es lo que se les pasa por la ...   archivo  \n",
       "4  “Estamos proponiendo la figura de un contrato ...   archivo  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Cargar el archivo de datos\n",
    "file_path = r\"C:\\Users\\Germán\\Documents\\machineLearning\\nlp_py_3_10\\Noticias.xlsx\"\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Mostrar una vista previa de los datos\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b11913e",
   "metadata": {},
   "source": [
    "## Paso 1: Preprocesamiento del Texto\n",
    "\n",
    "En este paso, transformaremos el texto a minúsculas, eliminaremos la puntuación, los números y los espacios en blanco adicionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f286e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contenido</th>\n",
       "      <th>contenido_preprocesado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Las autoridades están buscando al conductor de...</td>\n",
       "      <td>las autoridades están buscando al conductor de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Los andenes son para los peatones\", reclama e...</td>\n",
       "      <td>los andenes son para los peatones reclama enfá...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A las 7 de la noche me había quedado de encont...</td>\n",
       "      <td>a las  de la noche me había quedado de encontr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Atravesados. Eso es lo que se les pasa por la ...</td>\n",
       "      <td>atravesados eso es lo que se les pasa por la c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Estamos proponiendo la figura de un contrato ...</td>\n",
       "      <td>“estamos proponiendo la figura de un contrato ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           contenido  \\\n",
       "0  Las autoridades están buscando al conductor de...   \n",
       "1  \"Los andenes son para los peatones\", reclama e...   \n",
       "2  A las 7 de la noche me había quedado de encont...   \n",
       "3  Atravesados. Eso es lo que se les pasa por la ...   \n",
       "4  “Estamos proponiendo la figura de un contrato ...   \n",
       "\n",
       "                              contenido_preprocesado  \n",
       "0  las autoridades están buscando al conductor de...  \n",
       "1  los andenes son para los peatones reclama enfá...  \n",
       "2  a las  de la noche me había quedado de encontr...  \n",
       "3  atravesados eso es lo que se les pasa por la c...  \n",
       "4  “estamos proponiendo la figura de un contrato ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de los datos: (13738, 7)\n"
     ]
    }
   ],
   "source": [
    "# Función para preprocesar texto\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Esta función realiza el preprocesamiento del texto.\n",
    "    1. Convierte el texto a minúsculas.\n",
    "    2. Elimina la puntuación.\n",
    "    3. Elimina los números.\n",
    "    4. Elimina los espacios en blanco adicionales.\n",
    "\n",
    "    Parámetros:\n",
    "    text (str): El texto original.\n",
    "\n",
    "    Retorna:\n",
    "    str: El texto preprocesado.\n",
    "    \"\"\"\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # Eliminar puntuación\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Eliminar números\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Eliminar espacios en blanco adicionales\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Aplicar preprocesamiento al contenido\n",
    "## Deben eliminarse los valores nulos antes de aplicar el preprocesamiento\n",
    "data = data.dropna(subset=['contenido'])\n",
    "\n",
    "# Aplicar preprocesamiento al contenido\n",
    "data['contenido_preprocesado'] = data['contenido'].apply(preprocess_text)\n",
    "\n",
    "# Mostrar una vista previa de los datos preprocesados\n",
    "display(data[['contenido', 'contenido_preprocesado']].head())\n",
    "\n",
    "# Dimensiones de los datos\n",
    "\n",
    "print(f'Dimensiones de los datos: {data.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d6948",
   "metadata": {},
   "source": [
    "## Paso 2: Tokenización\n",
    "\n",
    "En este paso, convertiremos el texto preprocesado en una lista de palabras individuales utilizando la tokenización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c4b2be-890d-4001-9b0c-37b2b2403f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Columna1', 'Enlaces', 'Título', 'info', 'contenido', 'Etiqueta',\n",
      "       'contenido_preprocesado'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b70aeff2-e259-4552-a2b5-d9f5ac93dfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           contenido  \\\n",
      "0  Las autoridades están buscando al conductor de...   \n",
      "1  \"Los andenes son para los peatones\", reclama e...   \n",
      "2  A las 7 de la noche me había quedado de encont...   \n",
      "3  Atravesados. Eso es lo que se les pasa por la ...   \n",
      "4  “Estamos proponiendo la figura de un contrato ...   \n",
      "\n",
      "                              contenido_preprocesado  \n",
      "0  las autoridades están buscando al conductor de...  \n",
      "1  los andenes son para los peatones reclama enfá...  \n",
      "2  a las  de la noche me había quedado de encontr...  \n",
      "3  atravesados eso es lo que se les pasa por la c...  \n",
      "4  “estamos proponiendo la figura de un contrato ...  \n"
     ]
    }
   ],
   "source": [
    "# Asegurarse de que no haya valores nulos en la columna 'contenido'\n",
    "data['contenido'] = data['contenido'].fillna(\"\")  # Reemplaza nulos por cadenas vacías\n",
    "\n",
    "# Convertir todo a cadenas de texto\n",
    "data['contenido'] = data['contenido'].astype(str)\n",
    "\n",
    "# Aplicar preprocesamiento al contenido\n",
    "data['contenido_preprocesado'] = data['contenido'].apply(preprocess_text)\n",
    "\n",
    "# Verificar si se creó la columna\n",
    "print(data[['contenido', 'contenido_preprocesado']].head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df47b88e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contenido_preprocesado</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>las autoridades están buscando al conductor de...</td>\n",
       "      <td>[las, autoridades, están, buscando, al, conduc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>los andenes son para los peatones reclama enfá...</td>\n",
       "      <td>[los, andenes, son, para, los, peatones, recla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a las  de la noche me había quedado de encontr...</td>\n",
       "      <td>[a, las, de, la, noche, me, había, quedado, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atravesados eso es lo que se les pasa por la c...</td>\n",
       "      <td>[atravesados, eso, es, lo, que, se, les, pasa,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“estamos proponiendo la figura de un contrato ...</td>\n",
       "      <td>[“, estamos, proponiendo, la, figura, de, un, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              contenido_preprocesado  \\\n",
       "0  las autoridades están buscando al conductor de...   \n",
       "1  los andenes son para los peatones reclama enfá...   \n",
       "2  a las  de la noche me había quedado de encontr...   \n",
       "3  atravesados eso es lo que se les pasa por la c...   \n",
       "4  “estamos proponiendo la figura de un contrato ...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [las, autoridades, están, buscando, al, conduc...  \n",
       "1  [los, andenes, son, para, los, peatones, recla...  \n",
       "2  [a, las, de, la, noche, me, había, quedado, de...  \n",
       "3  [atravesados, eso, es, lo, que, se, les, pasa,...  \n",
       "4  [“, estamos, proponiendo, la, figura, de, un, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenización\n",
    "data['tokens'] = data['contenido_preprocesado'].apply(word_tokenize)\n",
    "\n",
    "# Mostrar una vista previa de los tokens\n",
    "data[['contenido_preprocesado', 'tokens']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de35d9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenido preprocesado:  las autoridades están buscando al conductor de una moto que le lanzó el casco a carlos alberto carmona operador de grúa de la empresa segrup  quien perdió por una hora el conocimiento tras la agresión en un video quedó registrado el momento en el que tanto el trabajador de la empresa como el motociclista se encuentran discutiendo cerca de la avenida villavicencio con gaitán cortés en el acalorado encuentro verbal ambos amagan con golpearse con los objetos que tienen en sus manos entonces el conductor de la moto arroja su casco contra el operador el hombre recibe el impacto en su cara por lo que s e desgonza y en la caída se golpea la cabeza con la grúa que conduce  el agredido perdió el conocimiento por cerca de una hora en ese instante el agresor se retira caminando del lugar y en repetidas ocasiones mira para atrás en donde está el operador en el suelo luego su compañero agarra la moto y huye del lugar y a pocos metros recoge al agresor las autoridades buscan al hombre que conducía la moto jto c marca pulsar color verde para que responda por la agresión que le generó al trabajador de  años de edad hacias las  de la mañana de este sábado la secretaría de movilidad a través de su cuenta de twitter rechazó este hecho mientras que la empresa operadora instaurará una denuncia penal por este hecho para que se realice una investigación bogotá valle del cauca  pm se han recogido cerca de  mil mercados en donatón valle solidario la meta es llegar a  mil mercados personas del común y empresas se  coronavirus en colombia  pm cuarentena en medellín el diario de lo que pasa en la ciudad el miércoles  de marzo comenzó el aislamiento obligatorio para evita  gabriel garcía márquez  pm falleció el mago dávila primer linotipista que tuvo garcía márquez barranquilla  pm alias el satánico busca la libertad por vencimiento de términos santander  pm hombre murió en un parque sin saber que estaba contagiado de covid\n",
      "Tokens:  ['las', 'autoridades', 'están', 'buscando', 'al', 'conductor', 'de', 'una', 'moto', 'que', 'le', 'lanzó', 'el', 'casco', 'a', 'carlos', 'alberto', 'carmona', 'operador', 'de', 'grúa', 'de', 'la', 'empresa', 'segrup', 'quien', 'perdió', 'por', 'una', 'hora', 'el', 'conocimiento', 'tras', 'la', 'agresión', 'en', 'un', 'video', 'quedó', 'registrado', 'el', 'momento', 'en', 'el', 'que', 'tanto', 'el', 'trabajador', 'de', 'la', 'empresa', 'como', 'el', 'motociclista', 'se', 'encuentran', 'discutiendo', 'cerca', 'de', 'la', 'avenida', 'villavicencio', 'con', 'gaitán', 'cortés', 'en', 'el', 'acalorado', 'encuentro', 'verbal', 'ambos', 'amagan', 'con', 'golpearse', 'con', 'los', 'objetos', 'que', 'tienen', 'en', 'sus', 'manos', 'entonces', 'el', 'conductor', 'de', 'la', 'moto', 'arroja', 'su', 'casco', 'contra', 'el', 'operador', 'el', 'hombre', 'recibe', 'el', 'impacto', 'en', 'su', 'cara', 'por', 'lo', 'que', 's', 'e', 'desgonza', 'y', 'en', 'la', 'caída', 'se', 'golpea', 'la', 'cabeza', 'con', 'la', 'grúa', 'que', 'conduce', 'el', 'agredido', 'perdió', 'el', 'conocimiento', 'por', 'cerca', 'de', 'una', 'hora', 'en', 'ese', 'instante', 'el', 'agresor', 'se', 'retira', 'caminando', 'del', 'lugar', 'y', 'en', 'repetidas', 'ocasiones', 'mira', 'para', 'atrás', 'en', 'donde', 'está', 'el', 'operador', 'en', 'el', 'suelo', 'luego', 'su', 'compañero', 'agarra', 'la', 'moto', 'y', 'huye', 'del', 'lugar', 'y', 'a', 'pocos', 'metros', 'recoge', 'al', 'agresor', 'las', 'autoridades', 'buscan', 'al', 'hombre', 'que', 'conducía', 'la', 'moto', 'jto', 'c', 'marca', 'pulsar', 'color', 'verde', 'para', 'que', 'responda', 'por', 'la', 'agresión', 'que', 'le', 'generó', 'al', 'trabajador', 'de', 'años', 'de', 'edad', 'hacias', 'las', 'de', 'la', 'mañana', 'de', 'este', 'sábado', 'la', 'secretaría', 'de', 'movilidad', 'a', 'través', 'de', 'su', 'cuenta', 'de', 'twitter', 'rechazó', 'este', 'hecho', 'mientras', 'que', 'la', 'empresa', 'operadora', 'instaurará', 'una', 'denuncia', 'penal', 'por', 'este', 'hecho', 'para', 'que', 'se', 'realice', 'una', 'investigación', 'bogotá', 'valle', 'del', 'cauca', 'pm', 'se', 'han', 'recogido', 'cerca', 'de', 'mil', 'mercados', 'en', 'donatón', 'valle', 'solidario', 'la', 'meta', 'es', 'llegar', 'a', 'mil', 'mercados', 'personas', 'del', 'común', 'y', 'empresas', 'se', 'coronavirus', 'en', 'colombia', 'pm', 'cuarentena', 'en', 'medellín', 'el', 'diario', 'de', 'lo', 'que', 'pasa', 'en', 'la', 'ciudad', 'el', 'miércoles', 'de', 'marzo', 'comenzó', 'el', 'aislamiento', 'obligatorio', 'para', 'evita', 'gabriel', 'garcía', 'márquez', 'pm', 'falleció', 'el', 'mago', 'dávila', 'primer', 'linotipista', 'que', 'tuvo', 'garcía', 'márquez', 'barranquilla', 'pm', 'alias', 'el', 'satánico', 'busca', 'la', 'libertad', 'por', 'vencimiento', 'de', 'términos', 'santander', 'pm', 'hombre', 'murió', 'en', 'un', 'parque', 'sin', 'saber', 'que', 'estaba', 'contagiado', 'de', 'covid']\n"
     ]
    }
   ],
   "source": [
    "## Revise los tokens para asegurarse de que el texto se haya tokenizado correctamente\n",
    "\n",
    "print(\"Contenido preprocesado: \", data['contenido_preprocesado'][0])\n",
    "print(\"Tokens: \", data['tokens'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb297fe5",
   "metadata": {},
   "source": [
    "## Paso 3: Eliminación de Stop Words\n",
    "\n",
    "En este paso, eliminaremos las stop words de los tokens generados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e17e5bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_sin_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[las, autoridades, están, buscando, al, conduc...</td>\n",
       "      <td>[autoridades, buscando, conductor, moto, lanzó...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[los, andenes, son, para, los, peatones, recla...</td>\n",
       "      <td>[andenes, peatones, reclama, enfática, carmenz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[a, las, de, la, noche, me, había, quedado, de...</td>\n",
       "      <td>[noche, quedado, encontrar, boris, siempre, si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[atravesados, eso, es, lo, que, se, les, pasa,...</td>\n",
       "      <td>[atravesados, pasa, cabeza, residentes, transe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[“, estamos, proponiendo, la, figura, de, un, ...</td>\n",
       "      <td>[“, proponiendo, figura, contrato, civil, unió...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [las, autoridades, están, buscando, al, conduc...   \n",
       "1  [los, andenes, son, para, los, peatones, recla...   \n",
       "2  [a, las, de, la, noche, me, había, quedado, de...   \n",
       "3  [atravesados, eso, es, lo, que, se, les, pasa,...   \n",
       "4  [“, estamos, proponiendo, la, figura, de, un, ...   \n",
       "\n",
       "                                tokens_sin_stopwords  \n",
       "0  [autoridades, buscando, conductor, moto, lanzó...  \n",
       "1  [andenes, peatones, reclama, enfática, carmenz...  \n",
       "2  [noche, quedado, encontrar, boris, siempre, si...  \n",
       "3  [atravesados, pasa, cabeza, residentes, transe...  \n",
       "4  [“, proponiendo, figura, contrato, civil, unió...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminar stop words\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "data['tokens_sin_stopwords'] = data['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "\n",
    "# Mostrar una vista previa de los tokens sin stop words\n",
    "data[['tokens', 'tokens_sin_stopwords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b574ffa",
   "metadata": {},
   "source": [
    "## Paso 4: Cálculo de TF-IDF\n",
    "\n",
    "En este paso, calcularemos la representación TF-IDF de los textos preprocesados. TF-IDF (Term Frequency-Inverse Document Frequency) es una técnica que pondera la importancia de una palabra en un documento en relación con un corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc795571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abajo</th>\n",
       "      <th>abandonar</th>\n",
       "      <th>abandono</th>\n",
       "      <th>abastecimiento</th>\n",
       "      <th>abierta</th>\n",
       "      <th>abiertas</th>\n",
       "      <th>abierto</th>\n",
       "      <th>abiertos</th>\n",
       "      <th>abogada</th>\n",
       "      <th>abogado</th>\n",
       "      <th>...</th>\n",
       "      <th>última</th>\n",
       "      <th>últimas</th>\n",
       "      <th>último</th>\n",
       "      <th>últimos</th>\n",
       "      <th>única</th>\n",
       "      <th>únicamente</th>\n",
       "      <th>único</th>\n",
       "      <th>únicos</th>\n",
       "      <th>úsuga</th>\n",
       "      <th>útil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abajo  abandonar  abandono  abastecimiento  abierta  abiertas  abierto  \\\n",
       "0    0.0        0.0       0.0             0.0      0.0       0.0      0.0   \n",
       "1    0.0        0.0       0.0             0.0      0.0       0.0      0.0   \n",
       "2    0.0        0.0       0.0             0.0      0.0       0.0      0.0   \n",
       "3    0.0        0.0       0.0             0.0      0.0       0.0      0.0   \n",
       "4    0.0        0.0       0.0             0.0      0.0       0.0      0.0   \n",
       "\n",
       "   abiertos  abogada  abogado  ...  última  últimas  último  últimos  \\\n",
       "0       0.0      0.0      0.0  ...     0.0      0.0     0.0      0.0   \n",
       "1       0.0      0.0      0.0  ...     0.0      0.0     0.0      0.0   \n",
       "2       0.0      0.0      0.0  ...     0.0      0.0     0.0      0.0   \n",
       "3       0.0      0.0      0.0  ...     0.0      0.0     0.0      0.0   \n",
       "4       0.0      0.0      0.0  ...     0.0      0.0     0.0      0.0   \n",
       "\n",
       "      única  únicamente  único  únicos  úsuga  útil  \n",
       "0  0.000000         0.0    0.0     0.0    0.0   0.0  \n",
       "1  0.000000         0.0    0.0     0.0    0.0   0.0  \n",
       "2  0.000000         0.0    0.0     0.0    0.0   0.0  \n",
       "3  0.071635         0.0    0.0     0.0    0.0   0.0  \n",
       "4  0.000000         0.0    0.0     0.0    0.0   0.0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unir los tokens en una sola cadena de texto para cada documento\n",
    "data['texto_sin_stopwords'] = data['tokens_sin_stopwords'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Calcular TF-IDF con un límite en el número de términos\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limitar a 5000 términos más frecuentes\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['texto_sin_stopwords'])\n",
    "\n",
    "# Convertir la matriz TF-IDF a un DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Mostrar una vista previa de la matriz TF-IDF\n",
    "tfidf_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46975efd",
   "metadata": {},
   "source": [
    "### ¿Qué ganamos con TF-IDF?\n",
    "\n",
    "- **Frecuencia de término (TF)**: Mide la frecuencia de una palabra en un documento. Si una palabra aparece muchas veces en un documento, es probable que sea importante para ese documento.\n",
    "- **Frecuencia inversa de documento (IDF)**: Mide la rareza de una palabra en un corpus. Si una palabra es común en muchos documentos, es menos informativa que una palabra rara.\n",
    "\n",
    "La fórmula de TF-IDF es:\n",
    "\n",
    "$$ \\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t) $$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $\\text{TF}(t, d)$ es la frecuencia de la palabra $t$ en el documento $d$.\n",
    "- $\\text{IDF}(t)$ es la frecuencia inversa de documento de la palabra $t$ en el corpus.\n",
    "\n",
    "pero, que fue lo que hicimos, en resumen, con TF-IDF, convertimos el texto en vectores numéricos que representan la importancia de las palabras en el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "529812c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En este ejemplo particular tomamos el contenido:\n",
      "\n",
      "las autoridades están buscando al conductor de una moto que le lanzó el casco a\n",
      "carlos alberto carmona operador de grúa de la empresa segrup quien perdió por una hora\n",
      "el conocimiento tras la agresión en un video quedó registrado el momento en el que\n",
      "tanto el trabajador de la empresa como el motociclista se encuentran discutiendo cerca de la\n",
      "avenida villavicencio con gaitán cortés en el acalorado encuentro verbal ambos amagan con golpearse con\n",
      "los objetos que tienen en sus manos entonces el conductor de la moto arroja su\n",
      "casco contra el operador el hombre recibe el impacto en su cara por lo que\n",
      "s e desgonza y en la caída se golpea la cabeza con la grúa que\n",
      "conduce el agredido perdió el conocimiento por cerca de una hora en ese instante el\n",
      "agresor se retira caminando del lugar y en repetidas ocasiones mira para atrás en donde\n",
      "está el operador en el suelo luego su compañero agarra la moto y huye del\n",
      "lugar y a pocos metros recoge al agresor las autoridades buscan al hombre que conducía\n",
      "la moto jto c marca pulsar color verde para que responda por la agresión que\n",
      "le generó al trabajador de años de edad hacias las de la mañana de este\n",
      "sábado la secretaría de movilidad a través de su cuenta de twitter rechazó este hecho\n",
      "mientras que la empresa operadora instaurará una denuncia penal por este hecho para que se\n",
      "realice una investigación bogotá valle del cauca pm se han recogido cerca de mil mercados\n",
      "en donatón valle solidario la meta es llegar a mil mercados personas del común y\n",
      "empresas se coronavirus en colombia pm cuarentena en medellín el diario de lo que pasa\n",
      "en la ciudad el miércoles de marzo comenzó el aislamiento obligatorio para evita gabriel garcía\n",
      "márquez pm falleció el mago dávila primer linotipista que tuvo garcía márquez barranquilla pm alias\n",
      "el satánico busca la libertad por vencimiento de términos santander pm hombre murió en un\n",
      "parque sin saber que estaba contagiado de covid \n",
      "\n",
      "Y lo convertimos en un vector TF-IDF de 119615 dimensiones:\n",
      "\n",
      "aa          0.0\n",
      "aaa         0.0\n",
      "aaacpt      0.0\n",
      "aaah        0.0\n",
      "aaas        0.0\n",
      "           ... \n",
      "𝑝𝑐𝑒𝑙𝑢𝑙𝑎𝑟    0.0\n",
      "𝑝𝑒𝑟𝑠𝑜𝑛𝑎     0.0\n",
      "𝑝𝑒𝑟𝑠𝑜𝑛𝑎𝑠    0.0\n",
      "𝑝𝑓𝑖𝑗𝑜       0.0\n",
      "𝑝𝑖          0.0\n",
      "Name: 0, Length: 119615, dtype: float64.\n",
      "\n",
      "\n",
      "Acabamos de convertir un documento de texto en un vector numérico que puede ser\n",
      "utilizado en algoritmos de aprendizaje automático. Ese vector representa la importancia \n",
      "de cada palabra en el documento original y nos permite usar por ejemplo:\n",
      "\n",
      "- ACP para reducir la dimensionalidad del vector.\n",
      "- Clustering para agrupar documentos similares.\n",
      "- Clasificación para predecir la categoría de un documento.\n",
      "- Recuperación de información para encontrar documentos similares.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Información adicional\n",
    "lista_contenido= data['contenido_preprocesado'][0].split()\n",
    "# Incluimos \\n cada 15 palabras para mejorar la legibilidad\n",
    "\n",
    "lista_contenido= [f\"{word} \" if (i+1)%15!=0 else f\"{word}\\n\" for i, word in enumerate(lista_contenido)]\n",
    "\n",
    "## Ahora convertimos la lista en un string\n",
    "\n",
    "contenido= ''.join(lista_contenido)\n",
    "\n",
    "text_info= f\"\"\"En este ejemplo particular tomamos el contenido:\n",
    "\n",
    "{contenido}\n",
    "\n",
    "Y lo convertimos en un vector TF-IDF de {tfidf_df.shape[1]} dimensiones:\n",
    "\n",
    "{tfidf_df.iloc[0]}.\n",
    "\n",
    "\n",
    "Acabamos de convertir un documento de texto en un vector numérico que puede ser\n",
    "utilizado en algoritmos de aprendizaje automático. Ese vector representa la importancia \n",
    "de cada palabra en el documento original y nos permite usar por ejemplo:\n",
    "\n",
    "- ACP para reducir la dimensionalidad del vector.\n",
    "- Clustering para agrupar documentos similares.\n",
    "- Clasificación para predecir la categoría de un documento.\n",
    "- Recuperación de información para encontrar documentos similares.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(text_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e4cb2",
   "metadata": {},
   "source": [
    "## Paso 5: Generación de Embeddings de Palabras con Word2Vec\n",
    "\n",
    "En este paso, utilizaremos el modelo Word2Vec para generar embeddings de palabras. Los embeddings de palabras son representaciones vectoriales densas que capturan el significado semántico de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebcd5b47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding de la palabra \"noticia\":\n",
      "[-0.9833666  -1.088931   -1.1311978  -0.31833443 -0.87410367 -1.6713638\n",
      " -0.18100783  1.010612   -0.96952885 -1.1092771   0.01467463  0.24098612\n",
      " -0.15478171 -0.8770411  -0.24025165 -0.32987714  0.05473063  0.48408443\n",
      " -0.31562075 -0.7104328   0.6987328   1.0090934  -0.5968329   0.20808616\n",
      "  0.33488867 -0.11779971 -1.5035694  -1.0969924   0.39711353  0.34023482\n",
      " -0.02926646  1.1552104  -0.19457567  0.10703279  0.26299238 -0.02548037\n",
      " -0.87725633  0.44233647 -0.2457657  -1.1846247  -0.5822891  -0.04241717\n",
      "  0.6332959  -0.18809061  0.54800665  0.05129688  1.4791838   0.23538622\n",
      "  1.0556158  -0.2647844   0.20821461  0.23181677  1.0510014  -0.46072423\n",
      " -0.15394688 -0.77402914  0.06931967  0.23655859  0.81294394  2.1543932\n",
      "  0.7070208   0.68633705  0.10116374 -0.41831145  0.61290675  0.69746286\n",
      "  0.16013949 -0.21511005 -0.47795036  0.00268865 -0.64672905  0.12361567\n",
      " -0.13366829 -0.5591385  -0.8010434   0.56552434  0.23836033  0.0439936\n",
      "  0.26260346 -0.8355913  -0.6579376   0.947262    0.06980387 -0.17116737\n",
      "  0.45365483 -0.65006405 -0.18839203 -0.01649418  0.82092875  0.631184\n",
      "  0.18933187 -1.1776671  -0.5382695   2.199147    0.9355406   1.0864227\n",
      "  0.9165085  -1.4093047   0.11128281 -0.12089164]\n",
      "Modelo guardado en: C:\\Users\\Germán\\Documents\\machineLearning\\nlp_py_3_10\\Embeddings\\word2vec.model\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo Word2Vec\n",
    "word2vec_model = Word2Vec(sentences=data['tokens_sin_stopwords'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Obtener los embeddings de una palabra ejemplo\n",
    "word_example = 'noticia'\n",
    "if word_example in word2vec_model.wv:\n",
    "    embedding_example = word2vec_model.wv[word_example]\n",
    "    print(f'Embedding de la palabra \"{word_example}\":\\n{embedding_example}')\n",
    "else:\n",
    "    print(f'La palabra \"{word_example}\" no está en el vocabulario del modelo Word2Vec.')\n",
    "\n",
    "# Ruta corregida\n",
    "save_dir = r\"C:\\Users\\Germán\\Documents\\machineLearning\\nlp_py_3_10\\Embeddings\"\n",
    "\n",
    "# Crear directorio \n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Guardar el modelo Word2Vec\n",
    "word2vec_model.save(os.path.join(save_dir, \"word2vec.model\"))\n",
    "\n",
    "print(f\"Modelo guardado en: {os.path.join(save_dir, 'word2vec.model')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23b2b83",
   "metadata": {},
   "source": [
    "## Guardar Resultados\n",
    "\n",
    "Finalmente, guardaremos los resultados preprocesados en un archivo CSV para su posterior uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07db4e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: C:\\Users\\Germán\\Documents\\machineLearning\\nlp_py_3_10\\Embeddings\\Noticias_preprocesadas.csv\n"
     ]
    }
   ],
   "source": [
    "# Guardar los resultados preprocesados\n",
    "#data.to_csv('../../Datos/Datos Preprocesados/Noticias_preprocesadas.csv', index=False)\n",
    "output_path = os.path.join(save_dir, \"Noticias_preprocesadas.csv\")\n",
    "data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Resultados guardados en: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ea944",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "### Ajuste Ejercicio\n",
    "\n",
    "Hemos creado dos modelos de vectorización de palabras, uno basado en TF-IDF y otro basado en Word2Vec. Su primera tarea es:\n",
    "\n",
    "1. Eliminar las palabras vacías del texto.\n",
    "        r: se hizo en pasos anteriores\n",
    "2. Calcular la representación TF-IDF de los textos preprocesados.\n",
    "3. Generar embeddings de palabras utilizando Word2Vec.\n",
    "\n",
    "Para la representación TF-IDF, utilice unigramas y bigramas con un rango de frecuencia de 0.1 a 0.9. Para Word2Vec, utilice un tamaño de ventana de 5 y un tamaño de vector de 100.\n",
    "\n",
    "Finalmente, guarde los resultados en un archivo CSV.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fbae47-aa07-4832-9c6f-6351261a4fcc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Crear un Modelo Simple de ACP y Clustering\n",
    "\n",
    "Después de generar los vectores de palabras, su segunda tarea es crear un modelo simple de ACP (Análisis de Componentes Principales) y aplicar clustering a los vectores generados. Utilice el algoritmo KMeans con 5 clusters.\n",
    "\n",
    "PASOS:\n",
    "\n",
    "1. Ajuste un modelo de ACP a los vectores generados.\n",
    "2. Ajuste un modelo de KMeans con 5 clusters a los componentes principales.\n",
    "3. Analice los resultados del clustering y determine si los clusters son significativos.\n",
    "\n",
    "### Preguntas\n",
    "\n",
    "1. ¿Qué puede inferir de los clusters generados?\n",
    "2. ¿Qué palabras son las más representativas de cada cluster?\n",
    "3. ¿Qué palabras tienen los embeddings más similares?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
