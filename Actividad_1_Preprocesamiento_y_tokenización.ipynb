{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64464af",
   "metadata": {},
   "source": [
    "# Actividad 1: Preprocesamiento y Tokenizaci√≥n\n",
    "---\n",
    "En esta actividad, realizaremos los siguientes pasos:\n",
    "\n",
    "1. Cargar el archivo de datos de noticias.\n",
    "2. Realizar el preprocesamiento del texto, que incluye:\n",
    "   - Convertir el texto a min√∫sculas.\n",
    "   - Eliminar puntuaci√≥n.\n",
    "   - Eliminar n√∫meros.\n",
    "   - Eliminar espacios en blanco adicionales.\n",
    "3. Tokenizar el texto en palabras individuales.\n",
    "4. Eliminar stop words del texto tokenizado.\n",
    "5. Calcular TF-IDF para representar el texto como vectores num√©ricos.\n",
    "6. Generar embeddings de palabras utilizando Word2Vec.\n",
    "\n",
    "\n",
    "## Librer√≠as\n",
    "\n",
    "Para esta actividad, necesitaremos las siguientes librer√≠as:\n",
    "\n",
    "- pandas: Para cargar y manipular los datos.\n",
    "- numpy: Para realizar operaciones num√©ricas.\n",
    "- nltk: Para realizar el preprocesamiento y tokenizaci√≥n del texto.\n",
    "- gensim: Para generar los embeddings de palabras.\n",
    "\n",
    "Este proyecto usa Python 3.10 y usa poetry para manejar las dependencias. Para instalar las dependencias, ejecute `poetry install` en la carpeta ra√≠z del proyecto. Para m√°s informaci√≥n sobre poetry, consulte la [documentaci√≥n oficial](https://python-poetry.org/docs/).\n",
    "\n",
    "Si no quiere usar poetry, puede instalar las dependencias manualmente usando pip:\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy scikit-learn nltk gensim scipy openpyxl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8aca610-e021-417a-8f4c-4265fe2ddb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.14\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9ac65",
   "metadata": {},
   "source": [
    "### NLTK\n",
    "\n",
    "Nltk requiere que descarguemos algunos recursos adicionales. Para hacerlo, ejecute el siguiente c√≥digo:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ee605c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Germ√°n\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Germ√°n\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Germ√°n\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Columna1</th>\n",
       "      <th>Enlaces</th>\n",
       "      <th>T√≠tulo</th>\n",
       "      <th>info</th>\n",
       "      <th>contenido</th>\n",
       "      <th>Etiqueta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.eltiempo.com/agresion-contra-un-op...</td>\n",
       "      <td>Operador de gr√∫a qued√≥ inconsciente tras agres...</td>\n",
       "      <td>El conductor de una moto le lanz√≥ el casco y p...</td>\n",
       "      <td>Las autoridades est√°n buscando al conductor de...</td>\n",
       "      <td>colombia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>Usaqu√©n, primera en infracciones por mal parqueo</td>\n",
       "      <td>La localidad ocupa el primer lugar en comparen...</td>\n",
       "      <td>\"Los andenes son para los peatones\", reclama e...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>'Me atracaron y vi un arma que me hel√≥ la sang...</td>\n",
       "      <td>Un ciudadano relata c√≥mo cuatro hombres lo rob...</td>\n",
       "      <td>A las 7 de la noche me hab√≠a quedado de encont...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>Escoltas mal estacionados, dolor de cabeza de ...</td>\n",
       "      <td>Las zonas de restaurantes se convierten en par...</td>\n",
       "      <td>Atravesados. Eso es lo que se les pasa por la ...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.eltiempo.com/archivo/documento/CMS...</td>\n",
       "      <td>Radicado primer proyecto que autorizar√≠a union...</td>\n",
       "      <td>El representante de 'la U', Miguel G√≥mez, dijo...</td>\n",
       "      <td>‚ÄúEstamos proponiendo la figura de un contrato ...</td>\n",
       "      <td>archivo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Columna1                                            Enlaces  \\\n",
       "0         0  https://www.eltiempo.com/agresion-contra-un-op...   \n",
       "1         1  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "2         2  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "3         3  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "4         4  https://www.eltiempo.com/archivo/documento/CMS...   \n",
       "\n",
       "                                              T√≠tulo  \\\n",
       "0  Operador de gr√∫a qued√≥ inconsciente tras agres...   \n",
       "1   Usaqu√©n, primera en infracciones por mal parqueo   \n",
       "2  'Me atracaron y vi un arma que me hel√≥ la sang...   \n",
       "3  Escoltas mal estacionados, dolor de cabeza de ...   \n",
       "4  Radicado primer proyecto que autorizar√≠a union...   \n",
       "\n",
       "                                                info  \\\n",
       "0  El conductor de una moto le lanz√≥ el casco y p...   \n",
       "1  La localidad ocupa el primer lugar en comparen...   \n",
       "2  Un ciudadano relata c√≥mo cuatro hombres lo rob...   \n",
       "3  Las zonas de restaurantes se convierten en par...   \n",
       "4  El representante de 'la U', Miguel G√≥mez, dijo...   \n",
       "\n",
       "                                           contenido  Etiqueta  \n",
       "0  Las autoridades est√°n buscando al conductor de...  colombia  \n",
       "1  \"Los andenes son para los peatones\", reclama e...   archivo  \n",
       "2  A las 7 de la noche me hab√≠a quedado de encont...   archivo  \n",
       "3  Atravesados. Eso es lo que se les pasa por la ...   archivo  \n",
       "4  ‚ÄúEstamos proponiendo la figura de un contrato ...   archivo  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Cargar el archivo de datos\n",
    "file_path = r\"C:\\Users\\Germ√°n\\Documents\\machineLearning\\nlp_py_3_10\\Noticias.xlsx\"\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Mostrar una vista previa de los datos\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b11913e",
   "metadata": {},
   "source": [
    "## Paso 1: Preprocesamiento del Texto\n",
    "\n",
    "En este paso, transformaremos el texto a min√∫sculas, eliminaremos la puntuaci√≥n, los n√∫meros y los espacios en blanco adicionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f286e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contenido</th>\n",
       "      <th>contenido_preprocesado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Las autoridades est√°n buscando al conductor de...</td>\n",
       "      <td>las autoridades est√°n buscando al conductor de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Los andenes son para los peatones\", reclama e...</td>\n",
       "      <td>los andenes son para los peatones reclama enf√°...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A las 7 de la noche me hab√≠a quedado de encont...</td>\n",
       "      <td>a las  de la noche me hab√≠a quedado de encontr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Atravesados. Eso es lo que se les pasa por la ...</td>\n",
       "      <td>atravesados eso es lo que se les pasa por la c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‚ÄúEstamos proponiendo la figura de un contrato ...</td>\n",
       "      <td>‚Äúestamos proponiendo la figura de un contrato ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           contenido  \\\n",
       "0  Las autoridades est√°n buscando al conductor de...   \n",
       "1  \"Los andenes son para los peatones\", reclama e...   \n",
       "2  A las 7 de la noche me hab√≠a quedado de encont...   \n",
       "3  Atravesados. Eso es lo que se les pasa por la ...   \n",
       "4  ‚ÄúEstamos proponiendo la figura de un contrato ...   \n",
       "\n",
       "                              contenido_preprocesado  \n",
       "0  las autoridades est√°n buscando al conductor de...  \n",
       "1  los andenes son para los peatones reclama enf√°...  \n",
       "2  a las  de la noche me hab√≠a quedado de encontr...  \n",
       "3  atravesados eso es lo que se les pasa por la c...  \n",
       "4  ‚Äúestamos proponiendo la figura de un contrato ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de los datos: (13738, 7)\n"
     ]
    }
   ],
   "source": [
    "# Funci√≥n para preprocesar texto\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Esta funci√≥n realiza el preprocesamiento del texto.\n",
    "    1. Convierte el texto a min√∫sculas.\n",
    "    2. Elimina la puntuaci√≥n.\n",
    "    3. Elimina los n√∫meros.\n",
    "    4. Elimina los espacios en blanco adicionales.\n",
    "\n",
    "    Par√°metros:\n",
    "    text (str): El texto original.\n",
    "\n",
    "    Retorna:\n",
    "    str: El texto preprocesado.\n",
    "    \"\"\"\n",
    "    # Convertir a min√∫sculas\n",
    "    text = text.lower()\n",
    "    # Eliminar puntuaci√≥n\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Eliminar n√∫meros\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Eliminar espacios en blanco adicionales\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Aplicar preprocesamiento al contenido\n",
    "## Deben eliminarse los valores nulos antes de aplicar el preprocesamiento\n",
    "data = data.dropna(subset=['contenido'])\n",
    "\n",
    "# Aplicar preprocesamiento al contenido\n",
    "data['contenido_preprocesado'] = data['contenido'].apply(preprocess_text)\n",
    "\n",
    "# Mostrar una vista previa de los datos preprocesados\n",
    "display(data[['contenido', 'contenido_preprocesado']].head())\n",
    "\n",
    "# Dimensiones de los datos\n",
    "\n",
    "print(f'Dimensiones de los datos: {data.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d6948",
   "metadata": {},
   "source": [
    "## Paso 2: Tokenizaci√≥n\n",
    "\n",
    "En este paso, convertiremos el texto preprocesado en una lista de palabras individuales utilizando la tokenizaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c4b2be-890d-4001-9b0c-37b2b2403f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Columna1', 'Enlaces', 'T√≠tulo', 'info', 'contenido', 'Etiqueta',\n",
      "       'contenido_preprocesado'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b70aeff2-e259-4552-a2b5-d9f5ac93dfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           contenido  \\\n",
      "0  Las autoridades est√°n buscando al conductor de...   \n",
      "1  \"Los andenes son para los peatones\", reclama e...   \n",
      "2  A las 7 de la noche me hab√≠a quedado de encont...   \n",
      "3  Atravesados. Eso es lo que se les pasa por la ...   \n",
      "4  ‚ÄúEstamos proponiendo la figura de un contrato ...   \n",
      "\n",
      "                              contenido_preprocesado  \n",
      "0  las autoridades est√°n buscando al conductor de...  \n",
      "1  los andenes son para los peatones reclama enf√°...  \n",
      "2  a las  de la noche me hab√≠a quedado de encontr...  \n",
      "3  atravesados eso es lo que se les pasa por la c...  \n",
      "4  ‚Äúestamos proponiendo la figura de un contrato ...  \n"
     ]
    }
   ],
   "source": [
    "# Asegurarse de que no haya valores nulos en la columna 'contenido'\n",
    "data['contenido'] = data['contenido'].fillna(\"\")  # Reemplaza nulos por cadenas vac√≠as\n",
    "\n",
    "# Convertir todo a cadenas de texto\n",
    "data['contenido'] = data['contenido'].astype(str)\n",
    "\n",
    "# Aplicar preprocesamiento al contenido\n",
    "data['contenido_preprocesado'] = data['contenido'].apply(preprocess_text)\n",
    "\n",
    "# Verificar si se cre√≥ la columna\n",
    "print(data[['contenido', 'contenido_preprocesado']].head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df47b88e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contenido_preprocesado</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>las autoridades est√°n buscando al conductor de...</td>\n",
       "      <td>[las, autoridades, est√°n, buscando, al, conduc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>los andenes son para los peatones reclama enf√°...</td>\n",
       "      <td>[los, andenes, son, para, los, peatones, recla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a las  de la noche me hab√≠a quedado de encontr...</td>\n",
       "      <td>[a, las, de, la, noche, me, hab√≠a, quedado, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atravesados eso es lo que se les pasa por la c...</td>\n",
       "      <td>[atravesados, eso, es, lo, que, se, les, pasa,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‚Äúestamos proponiendo la figura de un contrato ...</td>\n",
       "      <td>[‚Äú, estamos, proponiendo, la, figura, de, un, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              contenido_preprocesado  \\\n",
       "0  las autoridades est√°n buscando al conductor de...   \n",
       "1  los andenes son para los peatones reclama enf√°...   \n",
       "2  a las  de la noche me hab√≠a quedado de encontr...   \n",
       "3  atravesados eso es lo que se les pasa por la c...   \n",
       "4  ‚Äúestamos proponiendo la figura de un contrato ...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [las, autoridades, est√°n, buscando, al, conduc...  \n",
       "1  [los, andenes, son, para, los, peatones, recla...  \n",
       "2  [a, las, de, la, noche, me, hab√≠a, quedado, de...  \n",
       "3  [atravesados, eso, es, lo, que, se, les, pasa,...  \n",
       "4  [‚Äú, estamos, proponiendo, la, figura, de, un, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizaci√≥n\n",
    "data['tokens'] = data['contenido_preprocesado'].apply(word_tokenize)\n",
    "\n",
    "# Mostrar una vista previa de los tokens\n",
    "data[['contenido_preprocesado', 'tokens']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de35d9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenido preprocesado:  las autoridades est√°n buscando al conductor de una moto que le lanz√≥ el casco a carlos alberto carmona operador de gr√∫a de la empresa segrup  quien perdi√≥ por una hora el conocimiento tras la agresi√≥n en un video qued√≥ registrado el momento en el que tanto el trabajador de la empresa como el motociclista se encuentran discutiendo cerca de la avenida villavicencio con gait√°n cort√©s en el acalorado encuentro verbal ambos amagan con golpearse con los objetos que tienen en sus manos entonces el conductor de la moto arroja su casco contra el operador el hombre recibe el impacto en su cara por lo que s e desgonza y en la ca√≠da se golpea la cabeza con la gr√∫a que conduce  el agredido perdi√≥ el conocimiento por cerca de una hora en ese instante el agresor se retira caminando del lugar y en repetidas ocasiones mira para atr√°s en donde est√° el operador en el suelo luego su compa√±ero agarra la moto y huye del lugar y a pocos metros recoge al agresor las autoridades buscan al hombre que conduc√≠a la moto jto c marca pulsar color verde para que responda por la agresi√≥n que le gener√≥ al trabajador de  a√±os de edad hacias las  de la ma√±ana de este s√°bado la secretar√≠a de movilidad a trav√©s de su cuenta de twitter rechaz√≥ este hecho mientras que la empresa operadora instaurar√° una denuncia penal por este hecho para que se realice una investigaci√≥n bogot√° valle del cauca  pm se han recogido cerca de  mil mercados en donat√≥n valle solidario la meta es llegar a  mil mercados personas del com√∫n y empresas se  coronavirus en colombia  pm cuarentena en medell√≠n el diario de lo que pasa en la ciudad el mi√©rcoles  de marzo comenz√≥ el aislamiento obligatorio para evita  gabriel garc√≠a m√°rquez  pm falleci√≥ el mago d√°vila primer linotipista que tuvo garc√≠a m√°rquez barranquilla  pm alias el sat√°nico busca la libertad por vencimiento de t√©rminos santander  pm hombre muri√≥ en un parque sin saber que estaba contagiado de covid\n",
      "Tokens:  ['las', 'autoridades', 'est√°n', 'buscando', 'al', 'conductor', 'de', 'una', 'moto', 'que', 'le', 'lanz√≥', 'el', 'casco', 'a', 'carlos', 'alberto', 'carmona', 'operador', 'de', 'gr√∫a', 'de', 'la', 'empresa', 'segrup', 'quien', 'perdi√≥', 'por', 'una', 'hora', 'el', 'conocimiento', 'tras', 'la', 'agresi√≥n', 'en', 'un', 'video', 'qued√≥', 'registrado', 'el', 'momento', 'en', 'el', 'que', 'tanto', 'el', 'trabajador', 'de', 'la', 'empresa', 'como', 'el', 'motociclista', 'se', 'encuentran', 'discutiendo', 'cerca', 'de', 'la', 'avenida', 'villavicencio', 'con', 'gait√°n', 'cort√©s', 'en', 'el', 'acalorado', 'encuentro', 'verbal', 'ambos', 'amagan', 'con', 'golpearse', 'con', 'los', 'objetos', 'que', 'tienen', 'en', 'sus', 'manos', 'entonces', 'el', 'conductor', 'de', 'la', 'moto', 'arroja', 'su', 'casco', 'contra', 'el', 'operador', 'el', 'hombre', 'recibe', 'el', 'impacto', 'en', 'su', 'cara', 'por', 'lo', 'que', 's', 'e', 'desgonza', 'y', 'en', 'la', 'ca√≠da', 'se', 'golpea', 'la', 'cabeza', 'con', 'la', 'gr√∫a', 'que', 'conduce', 'el', 'agredido', 'perdi√≥', 'el', 'conocimiento', 'por', 'cerca', 'de', 'una', 'hora', 'en', 'ese', 'instante', 'el', 'agresor', 'se', 'retira', 'caminando', 'del', 'lugar', 'y', 'en', 'repetidas', 'ocasiones', 'mira', 'para', 'atr√°s', 'en', 'donde', 'est√°', 'el', 'operador', 'en', 'el', 'suelo', 'luego', 'su', 'compa√±ero', 'agarra', 'la', 'moto', 'y', 'huye', 'del', 'lugar', 'y', 'a', 'pocos', 'metros', 'recoge', 'al', 'agresor', 'las', 'autoridades', 'buscan', 'al', 'hombre', 'que', 'conduc√≠a', 'la', 'moto', 'jto', 'c', 'marca', 'pulsar', 'color', 'verde', 'para', 'que', 'responda', 'por', 'la', 'agresi√≥n', 'que', 'le', 'gener√≥', 'al', 'trabajador', 'de', 'a√±os', 'de', 'edad', 'hacias', 'las', 'de', 'la', 'ma√±ana', 'de', 'este', 's√°bado', 'la', 'secretar√≠a', 'de', 'movilidad', 'a', 'trav√©s', 'de', 'su', 'cuenta', 'de', 'twitter', 'rechaz√≥', 'este', 'hecho', 'mientras', 'que', 'la', 'empresa', 'operadora', 'instaurar√°', 'una', 'denuncia', 'penal', 'por', 'este', 'hecho', 'para', 'que', 'se', 'realice', 'una', 'investigaci√≥n', 'bogot√°', 'valle', 'del', 'cauca', 'pm', 'se', 'han', 'recogido', 'cerca', 'de', 'mil', 'mercados', 'en', 'donat√≥n', 'valle', 'solidario', 'la', 'meta', 'es', 'llegar', 'a', 'mil', 'mercados', 'personas', 'del', 'com√∫n', 'y', 'empresas', 'se', 'coronavirus', 'en', 'colombia', 'pm', 'cuarentena', 'en', 'medell√≠n', 'el', 'diario', 'de', 'lo', 'que', 'pasa', 'en', 'la', 'ciudad', 'el', 'mi√©rcoles', 'de', 'marzo', 'comenz√≥', 'el', 'aislamiento', 'obligatorio', 'para', 'evita', 'gabriel', 'garc√≠a', 'm√°rquez', 'pm', 'falleci√≥', 'el', 'mago', 'd√°vila', 'primer', 'linotipista', 'que', 'tuvo', 'garc√≠a', 'm√°rquez', 'barranquilla', 'pm', 'alias', 'el', 'sat√°nico', 'busca', 'la', 'libertad', 'por', 'vencimiento', 'de', 't√©rminos', 'santander', 'pm', 'hombre', 'muri√≥', 'en', 'un', 'parque', 'sin', 'saber', 'que', 'estaba', 'contagiado', 'de', 'covid']\n"
     ]
    }
   ],
   "source": [
    "## Revise los tokens para asegurarse de que el texto se haya tokenizado correctamente\n",
    "\n",
    "print(\"Contenido preprocesado: \", data['contenido_preprocesado'][0])\n",
    "print(\"Tokens: \", data['tokens'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb297fe5",
   "metadata": {},
   "source": [
    "## Paso 3: Eliminaci√≥n de Stop Words\n",
    "\n",
    "En este paso, eliminaremos las stop words de los tokens generados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e17e5bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_sin_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[las, autoridades, est√°n, buscando, al, conduc...</td>\n",
       "      <td>[autoridades, buscando, conductor, moto, lanz√≥...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[los, andenes, son, para, los, peatones, recla...</td>\n",
       "      <td>[andenes, peatones, reclama, enf√°tica, carmenz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[a, las, de, la, noche, me, hab√≠a, quedado, de...</td>\n",
       "      <td>[noche, quedado, encontrar, boris, siempre, si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[atravesados, eso, es, lo, que, se, les, pasa,...</td>\n",
       "      <td>[atravesados, pasa, cabeza, residentes, transe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[‚Äú, estamos, proponiendo, la, figura, de, un, ...</td>\n",
       "      <td>[‚Äú, proponiendo, figura, contrato, civil, uni√≥...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [las, autoridades, est√°n, buscando, al, conduc...   \n",
       "1  [los, andenes, son, para, los, peatones, recla...   \n",
       "2  [a, las, de, la, noche, me, hab√≠a, quedado, de...   \n",
       "3  [atravesados, eso, es, lo, que, se, les, pasa,...   \n",
       "4  [‚Äú, estamos, proponiendo, la, figura, de, un, ...   \n",
       "\n",
       "                                tokens_sin_stopwords  \n",
       "0  [autoridades, buscando, conductor, moto, lanz√≥...  \n",
       "1  [andenes, peatones, reclama, enf√°tica, carmenz...  \n",
       "2  [noche, quedado, encontrar, boris, siempre, si...  \n",
       "3  [atravesados, pasa, cabeza, residentes, transe...  \n",
       "4  [‚Äú, proponiendo, figura, contrato, civil, uni√≥...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminar stop words\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "data['tokens_sin_stopwords'] = data['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "\n",
    "# Mostrar una vista previa de los tokens sin stop words\n",
    "data[['tokens', 'tokens_sin_stopwords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b574ffa",
   "metadata": {},
   "source": [
    "## Paso 4: C√°lculo de TF-IDF\n",
    "\n",
    "En este paso, calcularemos la representaci√≥n TF-IDF de los textos preprocesados. TF-IDF (Term Frequency-Inverse Document Frequency) es una t√©cnica que pondera la importancia de una palabra en un documento en relaci√≥n con un corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc795571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abajo</th>\n",
       "      <th>abandonar</th>\n",
       "      <th>abandono</th>\n",
       "      <th>abastecimiento</th>\n",
       "      <th>abierta</th>\n",
       "      <th>abiertas</th>\n",
       "      <th>abierto</th>\n",
       "      <th>abiertos</th>\n",
       "      <th>abogada</th>\n",
       "      <th>abogado</th>\n",
       "      <th>...</th>\n",
       "      <th>√∫ltima</th>\n",
       "      <th>√∫ltimas</th>\n",
       "      <th>√∫ltimo</th>\n",
       "      <th>√∫ltimos</th>\n",
       "      <th>√∫nica</th>\n",
       "      <th>√∫nicamente</th>\n",
       "      <th>√∫nico</th>\n",
       "      <th>√∫nicos</th>\n",
       "      <th>√∫suga</th>\n",
       "      <th>√∫til</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abajo  abandonar  abandono  abastecimiento  abierta  abiertas  abierto  \\\n",
       "0    0.0        0.0       0.0             0.0      0.0       0.0      0.0   \n",
       "1    0.0        0.0       0.0             0.0      0.0       0.0      0.0   \n",
       "2    0.0        0.0       0.0             0.0      0.0       0.0      0.0   \n",
       "3    0.0        0.0       0.0             0.0      0.0       0.0      0.0   \n",
       "4    0.0        0.0       0.0             0.0      0.0       0.0      0.0   \n",
       "\n",
       "   abiertos  abogada  abogado  ...  √∫ltima  √∫ltimas  √∫ltimo  √∫ltimos  \\\n",
       "0       0.0      0.0      0.0  ...     0.0      0.0     0.0      0.0   \n",
       "1       0.0      0.0      0.0  ...     0.0      0.0     0.0      0.0   \n",
       "2       0.0      0.0      0.0  ...     0.0      0.0     0.0      0.0   \n",
       "3       0.0      0.0      0.0  ...     0.0      0.0     0.0      0.0   \n",
       "4       0.0      0.0      0.0  ...     0.0      0.0     0.0      0.0   \n",
       "\n",
       "      √∫nica  √∫nicamente  √∫nico  √∫nicos  √∫suga  √∫til  \n",
       "0  0.000000         0.0    0.0     0.0    0.0   0.0  \n",
       "1  0.000000         0.0    0.0     0.0    0.0   0.0  \n",
       "2  0.000000         0.0    0.0     0.0    0.0   0.0  \n",
       "3  0.071635         0.0    0.0     0.0    0.0   0.0  \n",
       "4  0.000000         0.0    0.0     0.0    0.0   0.0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unir los tokens en una sola cadena de texto para cada documento\n",
    "data['texto_sin_stopwords'] = data['tokens_sin_stopwords'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Calcular TF-IDF con un l√≠mite en el n√∫mero de t√©rminos\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limitar a 5000 t√©rminos m√°s frecuentes\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['texto_sin_stopwords'])\n",
    "\n",
    "# Convertir la matriz TF-IDF a un DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Mostrar una vista previa de la matriz TF-IDF\n",
    "tfidf_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46975efd",
   "metadata": {},
   "source": [
    "### ¬øQu√© ganamos con TF-IDF?\n",
    "\n",
    "- **Frecuencia de t√©rmino (TF)**: Mide la frecuencia de una palabra en un documento. Si una palabra aparece muchas veces en un documento, es probable que sea importante para ese documento.\n",
    "- **Frecuencia inversa de documento (IDF)**: Mide la rareza de una palabra en un corpus. Si una palabra es com√∫n en muchos documentos, es menos informativa que una palabra rara.\n",
    "\n",
    "La f√≥rmula de TF-IDF es:\n",
    "\n",
    "$$ \\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t) $$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $\\text{TF}(t, d)$ es la frecuencia de la palabra $t$ en el documento $d$.\n",
    "- $\\text{IDF}(t)$ es la frecuencia inversa de documento de la palabra $t$ en el corpus.\n",
    "\n",
    "pero, que fue lo que hicimos, en resumen, con TF-IDF, convertimos el texto en vectores num√©ricos que representan la importancia de las palabras en el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "529812c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En este ejemplo particular tomamos el contenido:\n",
      "\n",
      "las autoridades est√°n buscando al conductor de una moto que le lanz√≥ el casco a\n",
      "carlos alberto carmona operador de gr√∫a de la empresa segrup quien perdi√≥ por una hora\n",
      "el conocimiento tras la agresi√≥n en un video qued√≥ registrado el momento en el que\n",
      "tanto el trabajador de la empresa como el motociclista se encuentran discutiendo cerca de la\n",
      "avenida villavicencio con gait√°n cort√©s en el acalorado encuentro verbal ambos amagan con golpearse con\n",
      "los objetos que tienen en sus manos entonces el conductor de la moto arroja su\n",
      "casco contra el operador el hombre recibe el impacto en su cara por lo que\n",
      "s e desgonza y en la ca√≠da se golpea la cabeza con la gr√∫a que\n",
      "conduce el agredido perdi√≥ el conocimiento por cerca de una hora en ese instante el\n",
      "agresor se retira caminando del lugar y en repetidas ocasiones mira para atr√°s en donde\n",
      "est√° el operador en el suelo luego su compa√±ero agarra la moto y huye del\n",
      "lugar y a pocos metros recoge al agresor las autoridades buscan al hombre que conduc√≠a\n",
      "la moto jto c marca pulsar color verde para que responda por la agresi√≥n que\n",
      "le gener√≥ al trabajador de a√±os de edad hacias las de la ma√±ana de este\n",
      "s√°bado la secretar√≠a de movilidad a trav√©s de su cuenta de twitter rechaz√≥ este hecho\n",
      "mientras que la empresa operadora instaurar√° una denuncia penal por este hecho para que se\n",
      "realice una investigaci√≥n bogot√° valle del cauca pm se han recogido cerca de mil mercados\n",
      "en donat√≥n valle solidario la meta es llegar a mil mercados personas del com√∫n y\n",
      "empresas se coronavirus en colombia pm cuarentena en medell√≠n el diario de lo que pasa\n",
      "en la ciudad el mi√©rcoles de marzo comenz√≥ el aislamiento obligatorio para evita gabriel garc√≠a\n",
      "m√°rquez pm falleci√≥ el mago d√°vila primer linotipista que tuvo garc√≠a m√°rquez barranquilla pm alias\n",
      "el sat√°nico busca la libertad por vencimiento de t√©rminos santander pm hombre muri√≥ en un\n",
      "parque sin saber que estaba contagiado de covid \n",
      "\n",
      "Y lo convertimos en un vector TF-IDF de 119615 dimensiones:\n",
      "\n",
      "aa          0.0\n",
      "aaa         0.0\n",
      "aaacpt      0.0\n",
      "aaah        0.0\n",
      "aaas        0.0\n",
      "           ... \n",
      "ùëùùëêùëíùëôùë¢ùëôùëéùëü    0.0\n",
      "ùëùùëíùëüùë†ùëúùëõùëé     0.0\n",
      "ùëùùëíùëüùë†ùëúùëõùëéùë†    0.0\n",
      "ùëùùëìùëñùëóùëú       0.0\n",
      "ùëùùëñ          0.0\n",
      "Name: 0, Length: 119615, dtype: float64.\n",
      "\n",
      "\n",
      "Acabamos de convertir un documento de texto en un vector num√©rico que puede ser\n",
      "utilizado en algoritmos de aprendizaje autom√°tico. Ese vector representa la importancia \n",
      "de cada palabra en el documento original y nos permite usar por ejemplo:\n",
      "\n",
      "- ACP para reducir la dimensionalidad del vector.\n",
      "- Clustering para agrupar documentos similares.\n",
      "- Clasificaci√≥n para predecir la categor√≠a de un documento.\n",
      "- Recuperaci√≥n de informaci√≥n para encontrar documentos similares.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Informaci√≥n adicional\n",
    "lista_contenido= data['contenido_preprocesado'][0].split()\n",
    "# Incluimos \\n cada 15 palabras para mejorar la legibilidad\n",
    "\n",
    "lista_contenido= [f\"{word} \" if (i+1)%15!=0 else f\"{word}\\n\" for i, word in enumerate(lista_contenido)]\n",
    "\n",
    "## Ahora convertimos la lista en un string\n",
    "\n",
    "contenido= ''.join(lista_contenido)\n",
    "\n",
    "text_info= f\"\"\"En este ejemplo particular tomamos el contenido:\n",
    "\n",
    "{contenido}\n",
    "\n",
    "Y lo convertimos en un vector TF-IDF de {tfidf_df.shape[1]} dimensiones:\n",
    "\n",
    "{tfidf_df.iloc[0]}.\n",
    "\n",
    "\n",
    "Acabamos de convertir un documento de texto en un vector num√©rico que puede ser\n",
    "utilizado en algoritmos de aprendizaje autom√°tico. Ese vector representa la importancia \n",
    "de cada palabra en el documento original y nos permite usar por ejemplo:\n",
    "\n",
    "- ACP para reducir la dimensionalidad del vector.\n",
    "- Clustering para agrupar documentos similares.\n",
    "- Clasificaci√≥n para predecir la categor√≠a de un documento.\n",
    "- Recuperaci√≥n de informaci√≥n para encontrar documentos similares.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(text_info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e4cb2",
   "metadata": {},
   "source": [
    "## Paso 5: Generaci√≥n de Embeddings de Palabras con Word2Vec\n",
    "\n",
    "En este paso, utilizaremos el modelo Word2Vec para generar embeddings de palabras. Los embeddings de palabras son representaciones vectoriales densas que capturan el significado sem√°ntico de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebcd5b47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding de la palabra \"noticia\":\n",
      "[-0.9833666  -1.088931   -1.1311978  -0.31833443 -0.87410367 -1.6713638\n",
      " -0.18100783  1.010612   -0.96952885 -1.1092771   0.01467463  0.24098612\n",
      " -0.15478171 -0.8770411  -0.24025165 -0.32987714  0.05473063  0.48408443\n",
      " -0.31562075 -0.7104328   0.6987328   1.0090934  -0.5968329   0.20808616\n",
      "  0.33488867 -0.11779971 -1.5035694  -1.0969924   0.39711353  0.34023482\n",
      " -0.02926646  1.1552104  -0.19457567  0.10703279  0.26299238 -0.02548037\n",
      " -0.87725633  0.44233647 -0.2457657  -1.1846247  -0.5822891  -0.04241717\n",
      "  0.6332959  -0.18809061  0.54800665  0.05129688  1.4791838   0.23538622\n",
      "  1.0556158  -0.2647844   0.20821461  0.23181677  1.0510014  -0.46072423\n",
      " -0.15394688 -0.77402914  0.06931967  0.23655859  0.81294394  2.1543932\n",
      "  0.7070208   0.68633705  0.10116374 -0.41831145  0.61290675  0.69746286\n",
      "  0.16013949 -0.21511005 -0.47795036  0.00268865 -0.64672905  0.12361567\n",
      " -0.13366829 -0.5591385  -0.8010434   0.56552434  0.23836033  0.0439936\n",
      "  0.26260346 -0.8355913  -0.6579376   0.947262    0.06980387 -0.17116737\n",
      "  0.45365483 -0.65006405 -0.18839203 -0.01649418  0.82092875  0.631184\n",
      "  0.18933187 -1.1776671  -0.5382695   2.199147    0.9355406   1.0864227\n",
      "  0.9165085  -1.4093047   0.11128281 -0.12089164]\n",
      "Modelo guardado en: C:\\Users\\Germ√°n\\Documents\\machineLearning\\nlp_py_3_10\\Embeddings\\word2vec.model\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo Word2Vec\n",
    "word2vec_model = Word2Vec(sentences=data['tokens_sin_stopwords'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Obtener los embeddings de una palabra ejemplo\n",
    "word_example = 'noticia'\n",
    "if word_example in word2vec_model.wv:\n",
    "    embedding_example = word2vec_model.wv[word_example]\n",
    "    print(f'Embedding de la palabra \"{word_example}\":\\n{embedding_example}')\n",
    "else:\n",
    "    print(f'La palabra \"{word_example}\" no est√° en el vocabulario del modelo Word2Vec.')\n",
    "\n",
    "# Ruta corregida\n",
    "save_dir = r\"C:\\Users\\Germ√°n\\Documents\\machineLearning\\nlp_py_3_10\\Embeddings\"\n",
    "\n",
    "# Crear directorio \n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Guardar el modelo Word2Vec\n",
    "word2vec_model.save(os.path.join(save_dir, \"word2vec.model\"))\n",
    "\n",
    "print(f\"Modelo guardado en: {os.path.join(save_dir, 'word2vec.model')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23b2b83",
   "metadata": {},
   "source": [
    "## Guardar Resultados\n",
    "\n",
    "Finalmente, guardaremos los resultados preprocesados en un archivo CSV para su posterior uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07db4e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados guardados en: C:\\Users\\Germ√°n\\Documents\\machineLearning\\nlp_py_3_10\\Embeddings\\Noticias_preprocesadas.csv\n"
     ]
    }
   ],
   "source": [
    "# Guardar los resultados preprocesados\n",
    "#data.to_csv('../../Datos/Datos Preprocesados/Noticias_preprocesadas.csv', index=False)\n",
    "output_path = os.path.join(save_dir, \"Noticias_preprocesadas.csv\")\n",
    "data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Resultados guardados en: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ea944",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "### Ajuste Ejercicio\n",
    "\n",
    "Hemos creado dos modelos de vectorizaci√≥n de palabras, uno basado en TF-IDF y otro basado en Word2Vec. Su primera tarea es:\n",
    "\n",
    "1. Eliminar las palabras vac√≠as del texto.\n",
    "        r: se hizo en pasos anteriores\n",
    "2. Calcular la representaci√≥n TF-IDF de los textos preprocesados.\n",
    "3. Generar embeddings de palabras utilizando Word2Vec.\n",
    "\n",
    "Para la representaci√≥n TF-IDF, utilice unigramas y bigramas con un rango de frecuencia de 0.1 a 0.9. Para Word2Vec, utilice un tama√±o de ventana de 5 y un tama√±o de vector de 100.\n",
    "\n",
    "Finalmente, guarde los resultados en un archivo CSV.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fbae47-aa07-4832-9c6f-6351261a4fcc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Crear un Modelo Simple de ACP y Clustering\n",
    "\n",
    "Despu√©s de generar los vectores de palabras, su segunda tarea es crear un modelo simple de ACP (An√°lisis de Componentes Principales) y aplicar clustering a los vectores generados. Utilice el algoritmo KMeans con 5 clusters.\n",
    "\n",
    "PASOS:\n",
    "\n",
    "1. Ajuste un modelo de ACP a los vectores generados.\n",
    "2. Ajuste un modelo de KMeans con 5 clusters a los componentes principales.\n",
    "3. Analice los resultados del clustering y determine si los clusters son significativos.\n",
    "\n",
    "### Preguntas\n",
    "\n",
    "1. ¬øQu√© puede inferir de los clusters generados?\n",
    "2. ¬øQu√© palabras son las m√°s representativas de cada cluster?\n",
    "3. ¬øQu√© palabras tienen los embeddings m√°s similares?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
